<h1 align="center">Semin√°rio Destila√ß√£o - Artefatos</h1>

> Este reposit√≥rio documenta o conte√∫do e os artefatos utilizados no semin√°rio apresentado sobre **Destila√ß√£o de Conhecimento**.

> Semin√°rio ocorreu em: 04/06/2025 (por isso j√° foi arquivado o reposit√≥rio, sendo apenas para leitura)

---
## üìö Artigos Utilizados

1. **[Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)**  
   Artigo que introduz a t√©cnica de *knowledge distillation* com redes professor-aluno.

2. **[DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)**    
   Proposta de destila√ß√£o de modelos BERT com t√©cnicas de compress√£o em v√°rios n√≠veis.

3. **[A Survey on Knowledge Distillation of Large Language Models](https://arxiv.org/html/2402.13116v4)**   
   Pesquisa completa sobre Destila√ß√£o em LLMs (2024).

---
## üõ†Ô∏è Artefatos

- **Apresenta√ß√£o em slides**: [slides/](slides/)
- **Modelo POSCOMP**: [notebooks/poscomp/](notebooks/poscomp/)
- **Outros casos de Destila√ß√£o**: [MasterChef-AI](https://agents4good.github.io/MasterChef-AI/content/destilacao/)

---
## ü§ù Contribui√ß√µes

- Contribui√ß√µes s√£o bem-vindas!  
- Basta abrir uma *issue* ou enviar um *pull request*.
